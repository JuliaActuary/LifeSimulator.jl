# Benchmarking 

The below results are generated by the benchmarking scripts in the folders for each language. These scripts are run automatically by GitHub Actions and populate the results below. 

```yaml 
Julia:
  basic_term_benchmark:
    cf1:
      mean: TrialEstimate(835.620 ms)
  exposures:
    ExperienceAnalysis:
      mean: TrialEstimate(70.924 ms)
      num_rows: 143166
    ExperienceAnalysis fork:
      mean: TrialEstimate(42.376 ms)
      num_rows: 141281
  mortality:
    mortality1:
      mean: TrialEstimate(425.265 Î¼s)
      result: 1904.4865526636793
Python:
  basic_term_benchmark:
    basic_term_m_lifelib:
      mean: 1646.1651785 milliseconds
  mortality:
    mortality1:
      mean: 1572.0383679999998 milliseconds
      result: 1904.4865526636793
R:
  exposures:
    actxps:
      mean: 760.268577 ms
      num_rows: 141281
```